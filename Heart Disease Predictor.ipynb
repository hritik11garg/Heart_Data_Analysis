{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124c12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gargh\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43bb70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "#Importing the csv file\n",
    "dataset = pd.read_csv('heart.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd81157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the data by doing One Hot encoding and transforming\n",
    "dataset = pd.get_dummies(dataset,columns = ['sex','cp','fbs','restecg','exang','slope','ca','thal'])\n",
    "standarScaler = StandardScaler()\n",
    "columns_to_scale = ['age','trestbps','chol','thalach','oldpeak']\n",
    "dataset[columns_to_scale] = standarScaler.fit_transform(dataset[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8b31b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>target</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>sex_1</th>\n",
       "      <th>cp_0</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>...</th>\n",
       "      <th>slope_2</th>\n",
       "      <th>ca_0</th>\n",
       "      <th>ca_1</th>\n",
       "      <th>ca_2</th>\n",
       "      <th>ca_3</th>\n",
       "      <th>ca_4</th>\n",
       "      <th>thal_0</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.952197</td>\n",
       "      <td>0.763956</td>\n",
       "      <td>-0.256334</td>\n",
       "      <td>0.015443</td>\n",
       "      <td>1.087338</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.915313</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>0.072199</td>\n",
       "      <td>1.633471</td>\n",
       "      <td>2.122573</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.474158</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>-0.816773</td>\n",
       "      <td>0.977514</td>\n",
       "      <td>0.310912</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.180175</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>-0.198357</td>\n",
       "      <td>1.239897</td>\n",
       "      <td>-0.206705</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>-0.663867</td>\n",
       "      <td>2.082050</td>\n",
       "      <td>0.583939</td>\n",
       "      <td>-0.379244</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>0.478391</td>\n",
       "      <td>-0.101730</td>\n",
       "      <td>-1.165281</td>\n",
       "      <td>-0.724323</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-1.033002</td>\n",
       "      <td>-1.234996</td>\n",
       "      <td>0.342756</td>\n",
       "      <td>-0.771706</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.503641</td>\n",
       "      <td>0.706843</td>\n",
       "      <td>-1.029353</td>\n",
       "      <td>-0.378132</td>\n",
       "      <td>2.036303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>-2.227533</td>\n",
       "      <td>-1.515125</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.290464</td>\n",
       "      <td>-0.092738</td>\n",
       "      <td>-0.198357</td>\n",
       "      <td>1.064975</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  trestbps      chol   thalach   oldpeak  target  sex_0  sex_1  \\\n",
       "0    0.952197  0.763956 -0.256334  0.015443  1.087338       1      0      1   \n",
       "1   -1.915313 -0.092738  0.072199  1.633471  2.122573       1      0      1   \n",
       "2   -1.474158 -0.092738 -0.816773  0.977514  0.310912       1      1      0   \n",
       "3    0.180175 -0.663867 -0.198357  1.239897 -0.206705       1      0      1   \n",
       "4    0.290464 -0.663867  2.082050  0.583939 -0.379244       1      1      0   \n",
       "..        ...       ...       ...       ...       ...     ...    ...    ...   \n",
       "298  0.290464  0.478391 -0.101730 -1.165281 -0.724323       0      1      0   \n",
       "299 -1.033002 -1.234996  0.342756 -0.771706  0.138373       0      0      1   \n",
       "300  1.503641  0.706843 -1.029353 -0.378132  2.036303       0      0      1   \n",
       "301  0.290464 -0.092738 -2.227533 -1.515125  0.138373       0      0      1   \n",
       "302  0.290464 -0.092738 -0.198357  1.064975 -0.896862       0      1      0   \n",
       "\n",
       "     cp_0  cp_1  ...  slope_2  ca_0  ca_1  ca_2  ca_3  ca_4  thal_0  thal_1  \\\n",
       "0       0     0  ...        0     1     0     0     0     0       0       1   \n",
       "1       0     0  ...        0     1     0     0     0     0       0       0   \n",
       "2       0     1  ...        1     1     0     0     0     0       0       0   \n",
       "3       0     1  ...        1     1     0     0     0     0       0       0   \n",
       "4       1     0  ...        1     1     0     0     0     0       0       0   \n",
       "..    ...   ...  ...      ...   ...   ...   ...   ...   ...     ...     ...   \n",
       "298     1     0  ...        0     1     0     0     0     0       0       0   \n",
       "299     0     0  ...        0     1     0     0     0     0       0       0   \n",
       "300     1     0  ...        0     0     0     1     0     0       0       0   \n",
       "301     1     0  ...        0     0     1     0     0     0       0       0   \n",
       "302     0     1  ...        0     0     1     0     0     0       0       0   \n",
       "\n",
       "     thal_2  thal_3  \n",
       "0         0       0  \n",
       "1         1       0  \n",
       "2         1       0  \n",
       "3         1       0  \n",
       "4         1       0  \n",
       "..      ...     ...  \n",
       "298       0       1  \n",
       "299       0       1  \n",
       "300       0       1  \n",
       "301       0       1  \n",
       "302       1       0  \n",
       "\n",
       "[303 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91cd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in train and test\n",
    "y = dataset['target']\n",
    "X = dataset.drop(['target'],axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0657c6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Choice of k: 8\n"
     ]
    }
   ],
   "source": [
    "#calculating best values for knn classifier\n",
    "knn_scores = []\n",
    "for k in range(1,40):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_classifier.fit(X_train,y_train)\n",
    "    knn_scores.append(knn_classifier.score(X_test,y_test))\n",
    "print(f'Best Choice of k: {np.argmax(knn_scores)+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6006347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurace = 0.9180327868852459\n"
     ]
    }
   ],
   "source": [
    "#since we have 8 as best K value we are going to train noe with k = 8\n",
    "k = 8\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "knn_classifier.fit(X_train,y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "print(f'Accurace = {np.sum(y_pred == y_test)/len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0326761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice of c for linear: 1\n",
      "Best polynomial score: 5\n",
      "Best polynomial score: 5\n",
      "Best polynomial score: 6\n",
      "Best polynomial score: 3\n",
      "Best polynomial score: 7\n",
      "Best polynomial score: 7\n",
      "Best polynomial score: 7\n",
      "Best polynomial score: 7\n",
      "Best polynomial score: 7\n",
      "Best polynomial score: 7\n",
      "Best choice of c for poly: 1\n",
      "Best choice of c for rbf: 1\n",
      "Best choice of c for sigmoid: 1\n",
      "Best choice of k: rbf\n"
     ]
    }
   ],
   "source": [
    "#calculating best values for SVM\n",
    "svc_scores = []\n",
    "kernels = ['linear','poly','rbf','sigmoid']\n",
    "for i in range(len(kernels)):\n",
    "    svc_scores_c = []\n",
    "    for ch in range(1,11):\n",
    "        if kernels[i] == 'poly':\n",
    "            svc_scores_poly = []\n",
    "            for d in range(3,10):\n",
    "                svc_classifier = SVC(kernel = kernels[i], C = ch, degree=d)\n",
    "                svc_classifier.fit(X_train,y_train)\n",
    "                svc_scores_poly.append(svc_classifier.score(X_test,y_test))\n",
    "            print(f'Best polynomial score: {np.argmax(svc_scores_poly)+3}')\n",
    "            svc_scores_c.append(svc_scores_poly[np.argmax(svc_scores_poly)])\n",
    "        else:\n",
    "            svc_classifier = SVC(kernel=kernels[i],C=ch)\n",
    "            svc_classifier.fit(X_train,y_train)\n",
    "            svc_scores_c.append(svc_classifier.score(X_test,y_test))\n",
    "    print(f'Best choice of c for {kernels[i]}: {np.argmax(svc_scores_c)+1}')\n",
    "    svc_scores.append(svc_scores_c[np.argmax(svc_scores_c)])\n",
    "print(f'Best choice of k: {kernels[np.argmax(svc_scores)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73285d50",
   "metadata": {},
   "source": [
    "Now we know the best kernel for SVM is rbf with C value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08497abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "#Prediciting the score using best kernel for SVM\n",
    "svc_classifier = SVC(kernel='rbf',C=1)\n",
    "svc_classifier.fit(X_train,y_train)\n",
    "print(f'Score is {svc_classifier.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a98b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max features for gini: 14\n",
      "Best max features for entropy: 29\n",
      "Best criterion: gini\n"
     ]
    }
   ],
   "source": [
    "#calculating best values for Decision Tree\n",
    "cr_scores = []\n",
    "for cr in ['gini','entropy']:\n",
    "    dt_scores = []\n",
    "    for i in range(1,len(X.columns)+1):\n",
    "        dt_classifier = DecisionTreeClassifier(criterion=cr,max_features=i,random_state=42)\n",
    "        dt_classifier.fit(X_train,y_train)\n",
    "        dt_scores.append(dt_classifier.score(X_test,y_test))\n",
    "    print(f'Best max features for {cr}: {np.argmax(dt_scores)+1}')\n",
    "    cr_scores.append(dt_scores[np.argmax(dt_scores)])\n",
    "print(f'Best criterion: {\"gini\" if not np.argmax(cr_scores) else \"entropy\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c32d6",
   "metadata": {},
   "source": [
    "    We are gonna use gini for decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ebd70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "#Prediciting the score using best criteria i.e. gini\n",
    "dt_classifier =DecisionTreeClassifier(criterion='gini',max_features=14,random_state=42)\n",
    "dt_classifier.fit(X_train,y_train)\n",
    "print(dt_classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0177156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 501}\n"
     ]
    }
   ],
   "source": [
    "# Define a range of values for the number of estimators\n",
    "rf_param_grid = {\n",
    "    'n_estimators': range(1, 1000, 10)\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier object\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create a RandomizedSearchCV object with specified parameters\n",
    "rf_random = RandomizedSearchCV(\n",
    "    param_distributions=rf_param_grid,  # Use the defined parameter grid\n",
    "    estimator=rf,                      # Use the RandomForestClassifier\n",
    "    scoring=\"accuracy\",                # Use accuracy as the scoring metric\n",
    "    verbose=0,                         # Set verbosity level to 0 (no output during fitting)\n",
    "    n_iter=100,                        # Number of parameter settings that are sampled\n",
    "    cv=4                               # Number of folds for cross-validation\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object on the training data\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters found by the RandomizedSearchCV\n",
    "best_params = rf_random.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f'Best parameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ab1051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='feature'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function calculates feature importances using a given model and a DataFrame of features.\n",
    "def feature_imp(df, model):\n",
    "    # Create an empty DataFrame with columns 'feature' and 'importance'.\n",
    "    fi = pd.DataFrame(columns=['feature', 'importance'])\n",
    "    \n",
    "    # Set the 'feature' column to be the column names of the input DataFrame.\n",
    "    fi[\"feature\"] = df.columns\n",
    "    \n",
    "    # Set the 'importance' column to be the feature importances from the model.\n",
    "    fi[\"importance\"] = model.best_estimator_.feature_importances_\n",
    "    \n",
    "    # Return the DataFrame sorted by importance in descending order.\n",
    "    return fi.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Plot the feature importances in a horizontal bar chart.\n",
    "# The features will be on the y-axis and their importances on the x-axis.\n",
    "# The plot will have dimensions 10x7 inches.\n",
    "# The legend will be turned off.\n",
    "feature_imp(X_train, rf_random).plot('feature', 'importance', 'barh', figsize=(10,7), legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88a6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    # Add a dense layer with 20 units and ReLU activation function\n",
    "    layers.Dense(20, activation='relu', name='densel'),\n",
    "    \n",
    "    # Add a dropout layer with a dropout rate of 20%\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Add another dense layer with 25 units and ReLU activation function\n",
    "    layers.Dense(25, activation='relu', name='dense2'),\n",
    "    \n",
    "    # Add another dense layer with 45 units and ReLU activation function\n",
    "    layers.Dense(45, activation='relu', name='dense3'),\n",
    "    \n",
    "    # Add a dropout layer with a dropout rate of 50%\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Add another dense layer with 10 units and ReLU activation function\n",
    "    layers.Dense(10, activation='relu', name='dense4'),\n",
    "    \n",
    "    # Add a final dense layer with 2 units and sigmoid activation function\n",
    "    layers.Dense(2, activation='sigmoid', name='fc1'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b484af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras  # Import the Keras library from TensorFlow\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Define the loss function as SparseCategoricalCrossentropy\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),  # Define the optimizer as Adam with learning rate 0.001\n",
    "    metrics=['accuracy']  # Monitor the accuracy metric during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "490e912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gargh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 1s - loss: 0.6951 - accuracy: 0.5826 - 881ms/epoch - 110ms/step\n",
      "Epoch 2/100\n",
      "8/8 - 0s - loss: 0.6975 - accuracy: 0.5579 - 27ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "8/8 - 0s - loss: 0.6827 - accuracy: 0.5620 - 7ms/epoch - 856us/step\n",
      "Epoch 4/100\n",
      "8/8 - 0s - loss: 0.6677 - accuracy: 0.5950 - 27ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "8/8 - 0s - loss: 0.6561 - accuracy: 0.6033 - 11ms/epoch - 1ms/step\n",
      "Epoch 6/100\n",
      "8/8 - 0s - loss: 0.6500 - accuracy: 0.6157 - 11ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "8/8 - 0s - loss: 0.6269 - accuracy: 0.6694 - 11ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "8/8 - 0s - loss: 0.6212 - accuracy: 0.6901 - 11ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "8/8 - 0s - loss: 0.5938 - accuracy: 0.7314 - 11ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "8/8 - 0s - loss: 0.5861 - accuracy: 0.7273 - 12ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "8/8 - 0s - loss: 0.5532 - accuracy: 0.7686 - 11ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "8/8 - 0s - loss: 0.5586 - accuracy: 0.7521 - 11ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "8/8 - 0s - loss: 0.4848 - accuracy: 0.7934 - 11ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "8/8 - 0s - loss: 0.4582 - accuracy: 0.8058 - 13ms/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "8/8 - 0s - loss: 0.4542 - accuracy: 0.8264 - 17ms/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "8/8 - 0s - loss: 0.4250 - accuracy: 0.8140 - 14ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "8/8 - 0s - loss: 0.3762 - accuracy: 0.8306 - 22ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "8/8 - 0s - loss: 0.4014 - accuracy: 0.8347 - 14ms/epoch - 2ms/step\n",
      "Epoch 19/100\n",
      "8/8 - 0s - loss: 0.4016 - accuracy: 0.8512 - 11ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "8/8 - 0s - loss: 0.3815 - accuracy: 0.8512 - 17ms/epoch - 2ms/step\n",
      "Epoch 21/100\n",
      "8/8 - 0s - loss: 0.4060 - accuracy: 0.8430 - 11ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "8/8 - 0s - loss: 0.3831 - accuracy: 0.8388 - 2ms/epoch - 220us/step\n",
      "Epoch 23/100\n",
      "8/8 - 0s - loss: 0.3885 - accuracy: 0.8554 - 18ms/epoch - 2ms/step\n",
      "Epoch 24/100\n",
      "8/8 - 0s - loss: 0.3413 - accuracy: 0.8512 - 16ms/epoch - 2ms/step\n",
      "Epoch 25/100\n",
      "8/8 - 0s - loss: 0.3587 - accuracy: 0.8347 - 22ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "8/8 - 0s - loss: 0.3251 - accuracy: 0.8719 - 11ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "8/8 - 0s - loss: 0.3369 - accuracy: 0.8595 - 15ms/epoch - 2ms/step\n",
      "Epoch 28/100\n",
      "8/8 - 0s - loss: 0.3176 - accuracy: 0.8802 - 0s/epoch - 0s/step\n",
      "Epoch 29/100\n",
      "8/8 - 0s - loss: 0.3096 - accuracy: 0.8926 - 368us/epoch - 46us/step\n",
      "Epoch 30/100\n",
      "8/8 - 0s - loss: 0.3289 - accuracy: 0.8636 - 12ms/epoch - 2ms/step\n",
      "Epoch 31/100\n",
      "8/8 - 0s - loss: 0.2860 - accuracy: 0.8926 - 5ms/epoch - 640us/step\n",
      "Epoch 32/100\n",
      "8/8 - 0s - loss: 0.3044 - accuracy: 0.8760 - 19ms/epoch - 2ms/step\n",
      "Epoch 33/100\n",
      "8/8 - 0s - loss: 0.3252 - accuracy: 0.8678 - 14ms/epoch - 2ms/step\n",
      "Epoch 34/100\n",
      "8/8 - 0s - loss: 0.2965 - accuracy: 0.8926 - 0s/epoch - 0s/step\n",
      "Epoch 35/100\n",
      "8/8 - 0s - loss: 0.3143 - accuracy: 0.8926 - 12ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "8/8 - 0s - loss: 0.3364 - accuracy: 0.8843 - 4ms/epoch - 552us/step\n",
      "Epoch 37/100\n",
      "8/8 - 0s - loss: 0.3088 - accuracy: 0.8926 - 16ms/epoch - 2ms/step\n",
      "Epoch 38/100\n",
      "8/8 - 0s - loss: 0.2795 - accuracy: 0.9008 - 18ms/epoch - 2ms/step\n",
      "Epoch 39/100\n",
      "8/8 - 0s - loss: 0.2664 - accuracy: 0.8843 - 19ms/epoch - 2ms/step\n",
      "Epoch 40/100\n",
      "8/8 - 0s - loss: 0.2907 - accuracy: 0.8967 - 11ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "8/8 - 0s - loss: 0.2944 - accuracy: 0.8802 - 15ms/epoch - 2ms/step\n",
      "Epoch 42/100\n",
      "8/8 - 0s - loss: 0.2642 - accuracy: 0.8926 - 3ms/epoch - 313us/step\n",
      "Epoch 43/100\n",
      "8/8 - 0s - loss: 0.2849 - accuracy: 0.8926 - 16ms/epoch - 2ms/step\n",
      "Epoch 44/100\n",
      "8/8 - 0s - loss: 0.2789 - accuracy: 0.9008 - 16ms/epoch - 2ms/step\n",
      "Epoch 45/100\n",
      "8/8 - 0s - loss: 0.2912 - accuracy: 0.8843 - 26ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "8/8 - 0s - loss: 0.2645 - accuracy: 0.9050 - 5ms/epoch - 648us/step\n",
      "Epoch 47/100\n",
      "8/8 - 0s - loss: 0.2968 - accuracy: 0.8967 - 19ms/epoch - 2ms/step\n",
      "Epoch 48/100\n",
      "8/8 - 0s - loss: 0.2360 - accuracy: 0.9132 - 14ms/epoch - 2ms/step\n",
      "Epoch 49/100\n",
      "8/8 - 0s - loss: 0.2397 - accuracy: 0.9256 - 22ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "8/8 - 0s - loss: 0.2387 - accuracy: 0.9380 - 15ms/epoch - 2ms/step\n",
      "Epoch 51/100\n",
      "8/8 - 0s - loss: 0.2333 - accuracy: 0.9132 - 15ms/epoch - 2ms/step\n",
      "Epoch 52/100\n",
      "8/8 - 0s - loss: 0.2538 - accuracy: 0.9132 - 0s/epoch - 0s/step\n",
      "Epoch 53/100\n",
      "8/8 - 0s - loss: 0.2375 - accuracy: 0.9050 - 16ms/epoch - 2ms/step\n",
      "Epoch 54/100\n",
      "8/8 - 0s - loss: 0.2663 - accuracy: 0.9174 - 15ms/epoch - 2ms/step\n",
      "Epoch 55/100\n",
      "8/8 - 0s - loss: 0.2213 - accuracy: 0.9215 - 14ms/epoch - 2ms/step\n",
      "Epoch 56/100\n",
      "8/8 - 0s - loss: 0.2102 - accuracy: 0.9091 - 16ms/epoch - 2ms/step\n",
      "Epoch 57/100\n",
      "8/8 - 0s - loss: 0.2507 - accuracy: 0.9132 - 15ms/epoch - 2ms/step\n",
      "Epoch 58/100\n",
      "8/8 - 0s - loss: 0.2271 - accuracy: 0.9132 - 15ms/epoch - 2ms/step\n",
      "Epoch 59/100\n",
      "8/8 - 0s - loss: 0.2298 - accuracy: 0.9256 - 13ms/epoch - 2ms/step\n",
      "Epoch 60/100\n",
      "8/8 - 0s - loss: 0.2592 - accuracy: 0.8967 - 12ms/epoch - 2ms/step\n",
      "Epoch 61/100\n",
      "8/8 - 0s - loss: 0.2480 - accuracy: 0.9091 - 12ms/epoch - 2ms/step\n",
      "Epoch 62/100\n",
      "8/8 - 0s - loss: 0.2274 - accuracy: 0.9174 - 14ms/epoch - 2ms/step\n",
      "Epoch 63/100\n",
      "8/8 - 0s - loss: 0.2462 - accuracy: 0.9008 - 13ms/epoch - 2ms/step\n",
      "Epoch 64/100\n",
      "8/8 - 0s - loss: 0.2554 - accuracy: 0.9008 - 11ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "8/8 - 0s - loss: 0.2458 - accuracy: 0.9174 - 17ms/epoch - 2ms/step\n",
      "Epoch 66/100\n",
      "8/8 - 0s - loss: 0.2172 - accuracy: 0.9050 - 13ms/epoch - 2ms/step\n",
      "Epoch 67/100\n",
      "8/8 - 0s - loss: 0.2511 - accuracy: 0.9008 - 12ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "8/8 - 0s - loss: 0.2077 - accuracy: 0.9215 - 13ms/epoch - 2ms/step\n",
      "Epoch 69/100\n",
      "8/8 - 0s - loss: 0.2034 - accuracy: 0.9256 - 13ms/epoch - 2ms/step\n",
      "Epoch 70/100\n",
      "8/8 - 0s - loss: 0.1714 - accuracy: 0.9504 - 11ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "8/8 - 0s - loss: 0.2153 - accuracy: 0.9256 - 12ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "8/8 - 0s - loss: 0.1805 - accuracy: 0.9380 - 12ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "8/8 - 0s - loss: 0.2097 - accuracy: 0.9298 - 12ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "8/8 - 0s - loss: 0.2089 - accuracy: 0.9256 - 12ms/epoch - 2ms/step\n",
      "Epoch 75/100\n",
      "8/8 - 0s - loss: 0.1895 - accuracy: 0.9380 - 11ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "8/8 - 0s - loss: 0.2065 - accuracy: 0.9256 - 12ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "8/8 - 0s - loss: 0.1727 - accuracy: 0.9256 - 10ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "8/8 - 0s - loss: 0.2116 - accuracy: 0.9215 - 11ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "8/8 - 0s - loss: 0.1999 - accuracy: 0.9380 - 12ms/epoch - 2ms/step\n",
      "Epoch 80/100\n",
      "8/8 - 0s - loss: 0.1892 - accuracy: 0.9545 - 14ms/epoch - 2ms/step\n",
      "Epoch 81/100\n",
      "8/8 - 0s - loss: 0.1715 - accuracy: 0.9339 - 13ms/epoch - 2ms/step\n",
      "Epoch 82/100\n",
      "8/8 - 0s - loss: 0.2008 - accuracy: 0.9380 - 14ms/epoch - 2ms/step\n",
      "Epoch 83/100\n",
      "8/8 - 0s - loss: 0.1988 - accuracy: 0.9339 - 13ms/epoch - 2ms/step\n",
      "Epoch 84/100\n",
      "8/8 - 0s - loss: 0.1808 - accuracy: 0.9339 - 11ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "8/8 - 0s - loss: 0.1817 - accuracy: 0.9421 - 3ms/epoch - 329us/step\n",
      "Epoch 86/100\n",
      "8/8 - 0s - loss: 0.1819 - accuracy: 0.9174 - 24ms/epoch - 3ms/step\n",
      "Epoch 87/100\n",
      "8/8 - 0s - loss: 0.1686 - accuracy: 0.9339 - 4ms/epoch - 500us/step\n",
      "Epoch 88/100\n",
      "8/8 - 0s - loss: 0.1767 - accuracy: 0.9380 - 18ms/epoch - 2ms/step\n",
      "Epoch 89/100\n",
      "8/8 - 0s - loss: 0.1811 - accuracy: 0.9380 - 15ms/epoch - 2ms/step\n",
      "Epoch 90/100\n",
      "8/8 - 0s - loss: 0.2025 - accuracy: 0.9256 - 0s/epoch - 0s/step\n",
      "Epoch 91/100\n",
      "8/8 - 0s - loss: 0.1684 - accuracy: 0.9256 - 12ms/epoch - 1ms/step\n",
      "Epoch 92/100\n",
      "8/8 - 0s - loss: 0.1858 - accuracy: 0.9339 - 5ms/epoch - 622us/step\n",
      "Epoch 93/100\n",
      "8/8 - 0s - loss: 0.1472 - accuracy: 0.9463 - 18ms/epoch - 2ms/step\n",
      "Epoch 94/100\n",
      "8/8 - 0s - loss: 0.1628 - accuracy: 0.9421 - 16ms/epoch - 2ms/step\n",
      "Epoch 95/100\n",
      "8/8 - 0s - loss: 0.1644 - accuracy: 0.9339 - 17ms/epoch - 2ms/step\n",
      "Epoch 96/100\n",
      "8/8 - 0s - loss: 0.1841 - accuracy: 0.9421 - 13ms/epoch - 2ms/step\n",
      "Epoch 97/100\n",
      "8/8 - 0s - loss: 0.1473 - accuracy: 0.9380 - 2ms/epoch - 198us/step\n",
      "Epoch 98/100\n",
      "8/8 - 0s - loss: 0.1564 - accuracy: 0.9339 - 18ms/epoch - 2ms/step\n",
      "Epoch 99/100\n",
      "8/8 - 0s - loss: 0.1691 - accuracy: 0.9380 - 16ms/epoch - 2ms/step\n",
      "Epoch 100/100\n",
      "8/8 - 0s - loss: 0.1991 - accuracy: 0.9132 - 19ms/epoch - 2ms/step\n",
      "2/2 - 0s - loss: 0.6820 - accuracy: 0.8361 - 133ms/epoch - 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gargh\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6820310354232788, 0.8360655903816223]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training data (X_train and y_train)\n",
    "# Set the batch size to 32 and train for 100 epochs\n",
    "# Set verbose=2 to display progress during training\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=2)\n",
    "\n",
    "# Evaluate the model using the test data (X_test and y_test)\n",
    "# Set the batch size to 32 for evaluation\n",
    "# Set verbose=2 to display progress during evaluation\n",
    "model.evaluate(X_test, y_test, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c2a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss: 0.7482 - accuracy: 0.8525"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a326f",
   "metadata": {},
   "source": [
    "As we can see that the best technique in our case will be K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36d34f",
   "metadata": {},
   "source": [
    "I created random data myself and now going to test knn on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228af88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0    65    0   2       130   250    1        0      148      0      2.2   \n",
      "1    42    1   0       120   230    0        1      170      0      1.5   \n",
      "2    57    0   1       140   236    0        1      178      0      0.8   \n",
      "3    55    1   1       135   214    0        0      154      0      1.0   \n",
      "4    48    0   3       150   250    0        1      168      0      1.2   \n",
      "5    50    1   2       125   219    1        0      158      1      2.0   \n",
      "6    40    1   2       140   202    0        0      172      0      1.4   \n",
      "7    61    0   1       150   265    1        1      153      1      3.2   \n",
      "8    68    0   3       145   212    0        0      140      1      1.8   \n",
      "9    53    1   0       120   231    0        0      166      0      0.5   \n",
      "10   58    1   0       114   318    0        2      140      0      4.4   \n",
      "11   43    1   0       132   247    1        0      143      1      0.1   \n",
      "12   52    1   0       128   204    1        1      156      1      1.0   \n",
      "\n",
      "    slope  ca  thal  target  \n",
      "0       1   0     2       1  \n",
      "1       2   0     2       1  \n",
      "2       2   0     2       1  \n",
      "3       1   0     2       0  \n",
      "4       1   0     2       0  \n",
      "5       2   0     3       0  \n",
      "6       1   1     2       1  \n",
      "7       2   0     3       0  \n",
      "8       2   2     2       0  \n",
      "9       2   0     2       1  \n",
      "10      0   3     1       0  \n",
      "11      1   4     3       0  \n",
      "12      1   0     0       0  \n"
     ]
    }
   ],
   "source": [
    "# Define the data\n",
    "data = [\n",
    "    [65, 0, 2, 130, 250, 1, 0, 148, 0, 2.2, 1, 0, 2, 1],\n",
    "    [42, 1, 0, 120, 230, 0, 1, 170, 0, 1.5, 2, 0, 2, 1],\n",
    "    [57, 0, 1, 140, 236, 0, 1, 178, 0, 0.8, 2, 0, 2, 1],\n",
    "    [55, 1, 1, 135, 214, 0, 0, 154, 0, 1.0, 1, 0, 2, 0],\n",
    "    [48, 0, 3, 150, 250, 0, 1, 168, 0, 1.2, 1, 0, 2, 0],\n",
    "    [50, 1, 2, 125, 219, 1, 0, 158, 1, 2.0, 2, 0, 3, 0],\n",
    "    [40, 1, 2, 140, 202, 0, 0, 172, 0, 1.4, 1, 1, 2, 1],\n",
    "    [61, 0, 1, 150, 265, 1, 1, 153, 1, 3.2, 2, 0, 3, 0],\n",
    "    [68, 0, 3, 145, 212, 0, 0, 140, 1, 1.8, 2, 2, 2, 0],\n",
    "    [53, 1, 0, 120, 231, 0, 0, 166, 0, 0.5, 2, 0, 2, 1],\n",
    "    [58, 1, 0, 114, 318, 0, 2, 140, 0, 4.4, 0, 3, 1, 0],\n",
    "    [43, 1, 0, 132, 247, 1, 0, 143, 1, 0.1, 1, 4, 3, 0],\n",
    "    [52, 1, 0, 128, 204, 1, 1, 156, 1, 1, 1, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "column_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal','target'\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c28af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns = ['sex','cp','fbs','restecg','exang','slope','ca','thal'])\n",
    "standarScaler = StandardScaler()\n",
    "columns_to_scale = ['age','trestbps','chol','thalach','oldpeak']\n",
    "df[columns_to_scale] = standarScaler.fit_transform(df[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66927ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df['target']\n",
    "df_X = df.drop(['target'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dab1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data_pred = knn_classifier.predict(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de522482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d4eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "[1 1 1 0 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(random_data_pred.tolist())\n",
    "print(df_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "887e2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurace = 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "print(f'Accurace = {np.sum(random_data_pred == df_y)/len(df_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0a478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
